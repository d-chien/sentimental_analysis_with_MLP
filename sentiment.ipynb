{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document sentiment score: 0.41200000047683716\n",
      "Document sentiment magnitude: 1.7869999408721924\n",
      "Sentence text: 今天天氣真好。\n",
      "Sentence sentiment score: 0.9369999766349792\n",
      "Sentence sentiment magnitude: 0.9879999756813049\n",
      "Sentence text: 有花在綻放、鳥在唱歌，但身邊的學生卻苦命的要上學。\n",
      "Sentence sentiment score: -0.11100000143051147\n",
      "Sentence sentiment magnitude: 0.800000011920929\n",
      "Lanuage of the document: zh-Hant\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import language_v2\n",
    "\n",
    "client =  language_v2.LanguageServiceClient()\n",
    "doc = language_v2.Document.Type.PLAIN_TEXT\n",
    "language_code = \"zh-Hant\"\n",
    "document = {\n",
    "    \"content\": \"今天天氣真好。有花在綻放、鳥在唱歌，但身邊的學生卻苦命的要上學。\",\n",
    "    \"type\": doc,\n",
    "    \"language_code\": language_code\n",
    "}\n",
    "\n",
    "encoding_type = language_v2.EncodingType.UTF8\n",
    "\n",
    "response = client.analyze_sentiment(request={'document': document, 'encoding_type': encoding_type})\n",
    "print(f'Document sentiment score: {response.document_sentiment.score}')\n",
    "print(f'Document sentiment magnitude: {response.document_sentiment.magnitude}')\n",
    "\n",
    "for sentence in response.sentences:\n",
    "    print(f'Sentence text: {sentence.text.content}')\n",
    "    print(f'Sentence sentiment score: {sentence.sentiment.score}')\n",
    "    print(f'Sentence sentiment magnitude: {sentence.sentiment.magnitude}')\n",
    "\n",
    "print(f'Lanuage of the document: {response.language_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install \"numpy<2.0.0\" --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install numba --upgrade\n",
    "!pip install scipy --upgrade\n",
    "!pip install nltk --upgrade\n",
    "!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python3 -m spacy download zh_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install \"numpy<2.0.0\" --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "台灣是一個位於亞洲東部的島嶼國家。"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_zh = spacy.load(\"zh_core_web_trf\")\n",
    "doc = nlp_zh(\"台灣是一個位於亞洲東部的島嶼國家。\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos_</th>\n",
       "      <th>tag_</th>\n",
       "      <th>dep_</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>is_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>台灣</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NR</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>是</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VC</td>\n",
       "      <td>cop</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>一</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>nummod</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>個</td>\n",
       "      <td>NUM</td>\n",
       "      <td>M</td>\n",
       "      <td>mark:clf</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>位於</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VV</td>\n",
       "      <td>acl</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>亞洲</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NR</td>\n",
       "      <td>nmod:assmod</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>東部</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>dobj</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>的</td>\n",
       "      <td>PART</td>\n",
       "      <td>DEC</td>\n",
       "      <td>mark</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>島嶼</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>compound:nn</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>國家</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>。</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>PU</td>\n",
       "      <td>punct</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text   pos_ tag_         dep_  is_alpha  is_stop\n",
       "0    台灣  PROPN   NR        nsubj      True    False\n",
       "1     是   VERB   VC          cop      True     True\n",
       "2     一    NUM   CD       nummod      True     True\n",
       "3     個    NUM    M     mark:clf      True    False\n",
       "4    位於   VERB   VV          acl      True    False\n",
       "5    亞洲  PROPN   NR  nmod:assmod      True    False\n",
       "6    東部   NOUN   NN         dobj      True    False\n",
       "7     的   PART  DEC         mark      True     True\n",
       "8    島嶼   NOUN   NN  compound:nn      True    False\n",
       "9    國家   NOUN   NN         ROOT      True    False\n",
       "10    。  PUNCT   PU        punct     False     True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['text','pos_','tag_','dep_','is_alpha','is_stop']\n",
    "dim = list(map(lambda x: [x.text, x.pos_, x.tag_, x.dep_, x.is_alpha, x.is_stop], doc))\n",
    "pd.DataFrame(dim, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"zh\" id=\"6f5abbc97a8f490f83ff01ea70c6cd2f-0\" class=\"displacy\" width=\"850\" height=\"417.0\" direction=\"ltr\" style=\"max-width: none; height: 417.0px; color: white; background: black; font-family: ; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">台灣</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"130\">是</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"130\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"210\">一</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"210\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">個</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"370\">位於</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"370\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">亞洲</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">東部</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">的</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"690\">島嶼</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"690\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"327.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">國家。</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-0\" stroke-width=\"2px\" d=\"M62,282.0 62,188.66666666666669 770.0,188.66666666666669 770.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,284.0 L58,276.0 66,276.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-1\" stroke-width=\"2px\" d=\"M142,282.0 142,202.0 767.0,202.0 767.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cop</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M142,284.0 L138,276.0 146,276.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-2\" stroke-width=\"2px\" d=\"M222,282.0 222,215.33333333333331 764.0,215.33333333333331 764.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M222,284.0 L218,276.0 226,276.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-3\" stroke-width=\"2px\" d=\"M222,282.0 222,268.6666666666667 272.0,268.6666666666667 272.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark:clf</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M272.0,284.0 L276.0,276.0 268.0,276.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-4\" stroke-width=\"2px\" d=\"M382,282.0 382,228.66666666666666 761.0,228.66666666666666 761.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M382,284.0 L378,276.0 386,276.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-5\" stroke-width=\"2px\" d=\"M462,282.0 462,268.6666666666667 512.0,268.6666666666667 512.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod:assmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M462,284.0 L458,276.0 466,276.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-6\" stroke-width=\"2px\" d=\"M382,282.0 382,255.33333333333334 515.0,255.33333333333334 515.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M515.0,284.0 L519.0,276.0 511.0,276.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-7\" stroke-width=\"2px\" d=\"M382,282.0 382,242.0 598.0,242.0 598.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M598.0,284.0 L602.0,276.0 594.0,276.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-8\" stroke-width=\"2px\" d=\"M702,282.0 702,268.6666666666667 752.0,268.6666666666667 752.0,282.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f5abbc97a8f490f83ff01ea70c6cd2f-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound:nn</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M702,284.0 L698,276.0 706,276.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the dependency tree\n",
    "from spacy import displacy\n",
    "options = {\"compact\": True,\n",
    "           \"bg\":'black',\n",
    "           'color':'white',\n",
    "           'font':'',\n",
    "           'distance':80}\n",
    "displacy.render(doc, style='dep', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "台灣 GPE\n",
      "亞洲 LOC\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    台灣\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "是一個位於\n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    亞洲\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       "東部的島嶼國家。</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for e in doc.ents:\n",
    "    print(e.text, e.label_)\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \"國家\" should be ORG, but it is not recognized by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''因為國家訓練效果不佳，所以透過多個例句建立訓練資料，重新訓練“國家”這個實體類別'''\n",
    "# suggestion: use ChatGPT to generate more sentences\n",
    "training_data = [\n",
    "    (\"台灣的風景都非常美麗\",[(0,2,\"GPE\")]),\n",
    "    (\"亞洲東部國家的一些特色包括擁有豐富多彩的文化遺產和傳統習俗。\",[(0,4,\"LOC\"),(4,6,\"ORG\")]),\n",
    "    (\"在亞洲東部國家，食品文化有著獨特的地位，其美食吸引著眾多遊客前來品嚐。\",[(1,5,\"LOC\"),(5,7,\"ORG\")]),\n",
    "    (\"亞洲東部國家的經濟以製造業和出口為主，是全球經濟中的關鍵角色。\",[(0,4,\"LOC\"),(4,6,\"ORG\")]),\n",
    "    (\"在亞洲東部國家，教育非常重要，其高水平的教育體系吸引著世界各地的學生前來留學。\",[(3,5,\"LOC\"),(5,7,\"ORG\")]),\n",
    "    # (\"亞洲東部國家的一些城市，如東京、首爾和上海等，擁有先進的科技和發達的城市建設，是現代化的代表。\",[(2,4,\"LOC\"),(4,6,\"ORG\"),(13,15,\"ORG\"),(16,18,\"ORG\"),(19,21,\"ORG\")]),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 2016.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# train the NER model with training_data\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.blank(\"zh\")\n",
    "# DocBin is a container for Doc objects\n",
    "db = DocBin()\n",
    "for text, annot in tqdm(training_data):\n",
    "    _doc = nlp(text)\n",
    "    ents = []\n",
    "    for start, end, label in annot:\n",
    "        span = _doc.char_span(start, end, label=label)\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    _doc.ents = ents\n",
    "    db.add(_doc)\n",
    "db.to_disk(\"./train.spacy\")\n",
    "db.to_disk(\"./dev.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrain spacy model\n",
    "!python3 -m spacy init fill-config base_config.cfg config.cfg\n",
    "!python3 -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./dev.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    台灣\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "是一個位於\n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    亞洲東部\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       "的島嶼\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    國家\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "。</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = spacy.load(\"./output/model-best\")\n",
    "new_doc = best_model(\"台灣是一個位於亞洲東部的島嶼國家。\")\n",
    "displacy.render(new_doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentiment Analysis with Transformers\n",
    "#### follow [guide](<https://towardsdatascience.com/sentiment-analysis-with-transformers-a-complete-deep-learning-project-pt-i-d4ca7e47d676>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install keras\n",
    "!pip install keras-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, nltk, spacy, numpy as np, pandas as pd, tensorflow as tf, matplotlib.pyplot as plt, transformers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.metrics import Precision, Recall, AUC\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, CallbackList, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from keras.saving import register_keras_serializable\n",
    "from tensorflow.keras.layers import Layer, Dense\n",
    "from transformers import TFDistilBertModel, DistilBertConfig\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Train_data.txt', header=None,delimiter='\\t')\n",
    "test_data = pd.read_csv('Test_data.txt', header=None,delimiter='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149985, 2)\n",
      "(61998, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data.columns = ['sentiment','text']\n",
    "test_data.columns = ['sentiment','text']\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    75019\n",
      "1    74966\n",
      "Name: sentiment, dtype: int64\n",
      "1    31029\n",
      "0    30969\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data['sentiment'].value_counts())\n",
    "print(test_data['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python3 -m spacy download en_core_web_md -q\n",
    "!pip install \"numpy==1.26.0\" --force-reinstall -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x32b83f280>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower().strip() for token in doc if not token.is_stop]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run park fall .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\"I am running in the park and fell down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3118083917bf481aa015ea42d22df86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149985 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ce5fcdd3324d388874f2d684262e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Starting  back at work today   Looks like it'l...</td>\n",
       "      <td>start  work today  look like rain couple day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sugar levels dropping... munchies setting in. ...</td>\n",
       "      <td>sugar level drop ... munchie set . luckily bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>@karineb22 yeah!!! have a great summer break!</td>\n",
       "      <td>@karineb22 yeah ! ! ! great summer break !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>hannah montana was very good.  now going to re...</td>\n",
       "      <td>hannah montana good .  go read twilight .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>@Mayra326 aww, have fun!  I just had my 3D las...</td>\n",
       "      <td>@mayra326 aww , fun !  3d week !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text  \\\n",
       "0          0  Starting  back at work today   Looks like it'l...   \n",
       "1          1  Sugar levels dropping... munchies setting in. ...   \n",
       "2          1     @karineb22 yeah!!! have a great summer break!    \n",
       "3          1  hannah montana was very good.  now going to re...   \n",
       "4          1  @Mayra326 aww, have fun!  I just had my 3D las...   \n",
       "\n",
       "                                      processed_text  \n",
       "0       start  work today  look like rain couple day  \n",
       "1  sugar level drop ... munchie set . luckily bla...  \n",
       "2         @karineb22 yeah ! ! ! great summer break !  \n",
       "3          hannah montana good .  go read twilight .  \n",
       "4                   @mayra326 aww , fun !  3d week !  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "train_data['processed_text'] = train_data['text'].progress_apply(preprocess_text)\n",
    "test_data['processed_text'] = test_data['text'].progress_apply(preprocess_text)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_pickle('train_data.pkl')\n",
    "test_data.to_pickle('test_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsize in case of memory error\n",
    "train_data = pd.read_pickle('train_data.pkl')[:20000]\n",
    "test_data = pd.read_pickle('test_data.pkl')[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 7505), (20000, 7505))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first version - using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_data['processed_text'])\n",
    "test_tfidf = tfidf_vectorizer.transform(test_data['processed_text'])\n",
    "train_tfidf.shape, test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 7505), (20000, 7505))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_array = train_tfidf.toarray()\n",
    "X_test_array = test_tfidf.toarray()\n",
    "X_train_array.shape, X_test_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_tfidf, test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000,), (20000,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_le = train_data.sentiment\n",
    "y_test_le = test_data.sentiment\n",
    "y_train_le.shape, y_test_le.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99681021, 1.00321027])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imbalance class handling - assign greater weight to minority class\n",
    "class_weights = compute_class_weight('balanced', classes=y_train_le.unique(), y=y_train_le)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16000, 7505), (4000, 7505), (16000,), (4000,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_array, y_train_le, test_size=0.2, random_state=42, stratify=y_train_le)\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16000, 2), (4000, 2), (20000, 2))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_encoded = to_categorical(y_train)\n",
    "y_val_encoded = to_categorical(y_val)\n",
    "y_test_encoded = to_categorical(y_test_le)\n",
    "y_train_encoded.shape, y_val_encoded.shape, y_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convelutional, Bidirectional, attention, transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 4096)              30744576  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              8390656   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41299138 (157.54 MB)\n",
      "Trainable params: 41299138 (157.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# fully connected neural network\n",
    "model_v1 = Sequential()\n",
    "model_v1.add(Dense(4096, input_dim=X_train.shape[1], \n",
    "                   activation='selu', kernel_initializer='lecun_normal', kernel_regularizer=tf.keras.regularizers.l2(0.1)))\n",
    "model_v1.add(Dense(2048, activation='selu', kernel_initializer='lecun_normal',\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.1)))\n",
    "model_v1.add(Dense(1024, activation='selu', kernel_initializer='lecun_normal',\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.1)))\n",
    "model_v1.add(Dense(64, activation='selu'))\n",
    "# add output layer\n",
    "model_v1.add(Dense(2, activation='softmax'))\n",
    "# set up specific weight for each class\n",
    "model_v1.layers[-1].bias.assign(class_weights)\n",
    "\n",
    "# compile the model\n",
    "model_v1.compile(optimizer='Adam', loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy', Precision(), Recall(), AUC()])\n",
    "\n",
    "\n",
    "model_v1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lr_scheduler = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deployment\n",
    "'''as for the epoch, batch size, find the balance between the training time and the model performance, and hardware capability'''\n",
    "num_epochs = 20\n",
    "batch_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 15s 233ms/step - loss: 93.6297 - accuracy: 0.5295 - precision: 0.5295 - recall: 0.5295 - auc: 0.5535 - val_loss: 1.2176 - val_accuracy: 0.4985 - val_precision: 0.4985 - val_recall: 0.4985 - val_auc: 0.4985 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 14s 217ms/step - loss: 0.7818 - accuracy: 0.4997 - precision: 0.4997 - recall: 0.4997 - auc: 0.4990 - val_loss: 0.6954 - val_accuracy: 0.5015 - val_precision: 0.5015 - val_recall: 0.5015 - val_auc: 0.5015 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 14s 224ms/step - loss: 0.6943 - accuracy: 0.4959 - precision: 0.4959 - recall: 0.4959 - auc: 0.4945 - val_loss: 0.6937 - val_accuracy: 0.4985 - val_precision: 0.4985 - val_recall: 0.4985 - val_auc: 0.4985 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 14s 222ms/step - loss: 0.6935 - accuracy: 0.4984 - precision: 0.4984 - recall: 0.4984 - auc: 0.4984 - val_loss: 0.6934 - val_accuracy: 0.5015 - val_precision: 0.5015 - val_recall: 0.5015 - val_auc: 0.5015 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 14s 219ms/step - loss: 0.6938 - accuracy: 0.4950 - precision: 0.4950 - recall: 0.4950 - auc: 0.4946 - val_loss: 0.6932 - val_accuracy: 0.4985 - val_precision: 0.4985 - val_recall: 0.4985 - val_auc: 0.5000 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 14s 219ms/step - loss: 0.6936 - accuracy: 0.5011 - precision: 0.5011 - recall: 0.5011 - auc: 0.4991 - val_loss: 0.6932 - val_accuracy: 0.5015 - val_precision: 0.5015 - val_recall: 0.5015 - val_auc: 0.5015 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 14s 223ms/step - loss: 0.6934 - accuracy: 0.4981 - precision: 0.4981 - recall: 0.4981 - auc: 0.4991 - val_loss: 0.6933 - val_accuracy: 0.4985 - val_precision: 0.4985 - val_recall: 0.4985 - val_auc: 0.4985 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 14s 216ms/step - loss: 0.6932 - accuracy: 0.4980 - precision: 0.4980 - recall: 0.4980 - auc: 0.4996 - val_loss: 0.6932 - val_accuracy: 0.5015 - val_precision: 0.5015 - val_recall: 0.5015 - val_auc: 0.5015 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 14s 216ms/step - loss: 0.6933 - accuracy: 0.5019 - precision: 0.5019 - recall: 0.5019 - auc: 0.4996 - val_loss: 0.6931 - val_accuracy: 0.5015 - val_precision: 0.5015 - val_recall: 0.5015 - val_auc: 0.5000 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 14s 215ms/step - loss: 0.6932 - accuracy: 0.5035 - precision: 0.5035 - recall: 0.5035 - auc: 0.5039 - val_loss: 0.6933 - val_accuracy: 0.4985 - val_precision: 0.4985 - val_recall: 0.4985 - val_auc: 0.4985 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 14s 215ms/step - loss: 0.6933 - accuracy: 0.4956 - precision: 0.4956 - recall: 0.4956 - auc: 0.4923 - val_loss: 0.6932 - val_accuracy: 0.5015 - val_precision: 0.5015 - val_recall: 0.5015 - val_auc: 0.5015 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4970 - precision: 0.4970 - recall: 0.4970 - auc: 0.4915Restoring model weights from the end of the best epoch: 9.\n",
      "63/63 [==============================] - 13s 214ms/step - loss: 0.6932 - accuracy: 0.4970 - precision: 0.4970 - recall: 0.4970 - auc: 0.4915 - val_loss: 0.6931 - val_accuracy: 0.5015 - val_precision: 0.5015 - val_recall: 0.5015 - val_auc: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 12: early stopping\n",
      "CPU times: user 14min 24s, sys: 1min 6s, total: 15min 30s\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model_v1.fit(X_train, y_train_encoded, validation_data=(X_val, y_val_encoded),\n",
    "                        epochs=num_epochs, batch_size=batch_size, verbose=1,\n",
    "                        callbacks=[lr_scheduler, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 9s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "pred=model_v1.predict(X_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67     10013\n",
      "           1       0.00      0.00      0.00      9987\n",
      "\n",
      "    accuracy                           0.50     20000\n",
      "   macro avg       0.25      0.50      0.33     20000\n",
      "weighted avg       0.25      0.50      0.33     20000\n",
      "\n",
      "[[10013     0]\n",
      " [ 9987     0]]\n",
      "0.50065\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_encoded.argmax(axis=1), pred.argmax(axis=1)))\n",
    "print(confusion_matrix(y_test_encoded.argmax(axis=1), pred.argmax(axis=1)))\n",
    "print(accuracy_score(y_test_encoded.argmax(axis=1), pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
